# 机器学习



<img src="https://radiantheart.oss-cn-guangzhou.aliyuncs.com/myimage/20250203172102823.png" alt="56d5ba478553bbc95e6e09968eab1a5" style="zoom:33%;" />

一般分为监督学习与半监督学习

其中监督学习：通过一系列标准输入输出数据，产生最佳的模拟函数，到达输入input-----函数给出最佳output

分为两大类：一类是回归(regression)(比如高中常学的拟合函数),一类是分类(classification)，分类是无限输入-----有限输出，比如西瓜书中的好瓜还是坏瓜，肿瘤大小-------肿瘤良性or恶性。



<img src="https://radiantheart.oss-cn-guangzhou.aliyuncs.com/myimage/20250203173908444.png" alt="image-20250203173908268" style="zoom: 25%;" />



<img src="https://radiantheart.oss-cn-guangzhou.aliyuncs.com/myimage/20250204164005523.png" alt="image-20250204164005370" style="zoom:25%;" />

<img src="https://radiantheart.oss-cn-guangzhou.aliyuncs.com/myimage/20250204165551413.png" alt="image-20250204165551249" style="zoom:25%;" />

半监督学习：在未贴上标签的数据中发现一些特定的特征or结构

比如:clustering(聚类)，将没有标签的数据自动归类放置到不同集群中，如新闻集群：含关键词“熊猫”，“双胞胎”，“动物园”。



## 无监督学习（unsupervised learning）

输入数据没有被标记，也没有确定的结果。样本数据类别未知，需要根据样本间的相似性对样本集进行分类（聚类，clustering）试图使类内差距最小化，类间差距最大化。通俗点将就是实际应用中，不少情况下无法预先知道样本的标签，也就是说没有训练样本对应的类别，因而只能从原先没有样本标签的样本集开始学习分类器设计。

非监督学习目标不是告诉计算机怎么做，而是让它（计算机）自己去学习怎样做事情。非监督学习有两种思路。第一种思路是在指导Agent时不为其指定明确分类，而是在成功时，采用某种形式的激励制度。需要注意的是，这类训练通常会置于决策问题的框架里，因为它的目标不是为了产生一个分类系统，而是做出最大回报的决定，这种思路很好的概括了现实世界，agent可以对正确的行为做出激励，而对错误行为做出惩罚。

无监督学习的方法分为两大类：

(1)  一类为基于概率密度函数估计的直接方法：指设法找到各类别在特征空间的分布参数，再进行分类。

(2)  另一类是称为基于样本间相似性度量的简洁聚类方法：其原理是设法定出不同类别的核心或初始内核，然后依据样本与核心之间的相似性度量将样本聚集成不同的类别。

利用聚类结果，可以提取数据集中隐藏信息，对未来数据进行分类和预测。应用于数据挖掘，模式识别，图像处理等。

  PCA和很多deep learning算法都属于无监督学习。 

## 两者的不同点

1. 有监督学习方法必须要有训练集与测试样本。在训练集中找规律，而对测试样本使用这种规律。而非监督学习没有训练集，只有一组数据，在该组数据集内寻找规律。
2. 有监督学习的方法就是识别事物，识别的结果表现在给待识别数据加上了标签。因此训练样本集必须由带标签的样本组成。而非监督学习方法只有要分析的数据集的本身，预先没有什么标签。如果发现数据集呈现某种聚集性，则可按自然的聚集性分类，但不予以某种预先分类标签对上号为目的。
3. 非监督学习方法在寻找数据集中的规律性，这种规律性并不一定要达到划分数据集的目的，也就是说不一定要“分类”。

这一点是比有监督学习方法的用途要广。  譬如分析一堆数据的主分量，或分析数据集有什么特点都可以归于非监督学习方法的范畴。

4. 用非监督学习方法分析数据集的主分量与用K-L变换计算数据集的主分量又有区别。后者从方法上讲不是学习方法。因此用K-L变换找主分量不属于无监督学习方法，即方法上不是。而通过学习逐渐找到规律性这体现了学习方法这一点。在人工神经元网络中寻找主分量的方法属于无监督学习方法。 

## 何时采用哪种方法

 简单的方法就是从定义入手，有训练样本则考虑采用监督学习方法；无训练样本，则一定不能用监督学习方法。但是，现实问题中，即使没有训练样本，我们也能够凭借自己的双眼，从待分类的数据中，人工标注一些样本，并把它们作为训练样本，这样的话，可以把条件改善，用监督学习方法来做。对于不同的场景，正负样本的分布如果会存在偏移（可能大的偏移，可能比较小），这样的话，监督学习的效果可能就不如用非监督学习了。

<img src="https://radiantheart.oss-cn-guangzhou.aliyuncs.com/myimage/20250205092311010.png" alt="image-20250205092310802" style="zoom: 25%;" />

无监督学习：聚类学习，降维算法，异常检测



<img src="https://radiantheart.oss-cn-guangzhou.aliyuncs.com/myimage/20250205211024337.png" alt="image-20250205211024175" style="zoom:25%;" />

**聚类算法||||||||||||||||||||||均值聚类**

K[均值聚类](https://zhida.zhihu.com/search?content_id=101923999&content_type=Article&match_order=2&q=均值聚类&zhida_source=entity)就是制定分组的数量为K，自动进行分组。

K 均值聚类的步骤如下：

1. 定义 K 个重心。一开始这些重心是随机的（也有一些更加有效的用于初始化重心的算法）
2. 寻找最近的重心并且更新[聚类分配](https://zhida.zhihu.com/search?content_id=101923999&content_type=Article&match_order=1&q=聚类分配&zhida_source=entity)。将每个数据点都分配给这 K 个聚类中的一个。每个数据点都被分配给离它们最近的重心的聚类。这里的「接近程度」的度量是一个超参数——通常是[欧几里得距离](https://zhida.zhihu.com/search?content_id=101923999&content_type=Article&match_order=1&q=欧几里得距离&zhida_source=entity)（Euclidean distance）。
3. 将重心移动到它们的聚类的中心。每个聚类的重心的新位置是通过计算该[聚类](https://zhida.zhihu.com/search?content_id=101923999&content_type=Article&match_order=16&q=聚类&zhida_source=entity)中所有数据点的平均位置得到的。

重复第 2 和 3 步，直到每次迭代时重心的位置不再显著变化（即直到该算法收敛）







## 梯度下降

寻找多元导数最小值的实质

<img src="https://radiantheart.oss-cn-guangzhou.aliyuncs.com/myimage/20250206200451998.png" alt="41bb7d30d90bcd744e7047cb4d98ab4" style="zoom:33%;" />

<img src="https://radiantheart.oss-cn-guangzhou.aliyuncs.com/myimage/20250206200508200.png" alt="0338560611f46b8666f4eefe825df6a" style="zoom:33%;" />

关键点：寻找到处在位置下降最快的方向。

梯度下降算法又通常称为批量梯度下降算法。批量梯度下降每次学习都使用整个训练集，因此这些计算是冗余的，因为每次都使用完全相同的样本集。但其优点在于每次更新都会朝着正确的方向进行，最后能够保证收敛于极值点(凸函数收敛于全局极值点，非凸函数可能会收敛于局部极值点)，但是其缺点在于每次学习时间过长，如果训练集很大以至于需要消耗大量的内存，并且全量梯度下降不能进行在线模型参数更新。

training set(训练集)

<img src="https://radiantheart.oss-cn-guangzhou.aliyuncs.com/myimage/20250207164338566.png" alt="image-20250207164338396" style="zoom:33%;" />

<img src="https://radiantheart.oss-cn-guangzhou.aliyuncs.com/myimage/20250207165008989.png" alt="image-20250207165008609" style="zoom:25%;" />

![image-20250207165828998](https://radiantheart.oss-cn-guangzhou.aliyuncs.com/myimage/20250207165829177.png)

代价曲线的计算演示，通过j（w）——w图寻找代价最小的参数

<img src="https://radiantheart.oss-cn-guangzhou.aliyuncs.com/myimage/20250207170346454.png" alt="image-20250207170346124" style="zoom: 25%;" />



<img src="https://radiantheart.oss-cn-guangzhou.aliyuncs.com/myimage/20250207171105730.png" alt="image-20250207171105549" style="zoom: 33%;" />

<img src="https://radiantheart.oss-cn-guangzhou.aliyuncs.com/myimage/20250207172350705.png" alt="image-20250207172350268" style="zoom: 25%;" />

3D降维成2D的一种绘图，如图的三个不同颜色的线，他们在代价曲线的登高线上，意味着不同的参数组合也会导致相同的代价值，而最中间位置的点便是最小误差的参数组合







过拟合是指训练误差和测试误差之间的差距太大。换句换说，就是模型复杂度高于实际问题，模型在训练集上表现很好，但在测试集上却表现很差。模型对训练集"死记硬背"（记住了不适用于测试集的训练集性质或特点），没有理解数据背后的规律，泛化能力差。



1. 增加训练数据数

•   发生过拟合最常见的现象就是数据量太少而模型太复杂

•   过拟合是由于模型学习到了数据的一些噪声特征导致，增加训练数据的量能够减少噪声的影响，让模型更多地学习数据的一般特征

•   增加数据量有时可能不是那么容易，需要花费一定的时间和精力去搜集处理数据

•   利用现有数据进行扩充或许也是一个好办法。例如在图像识别中，如果没有足够的图片训练，可以把已有的图片进行旋转，拉伸，镜像，对称等，这样就可以把数据量扩大好几倍而不需要额外补充数据

•   注意保证训练数据的分布和测试数据的分布要保持一致，二者要是分布完全不同，那模型预测真可谓是对牛弹琴了

2. 使用正则化约束

•   在代价函数后面添加正则化项，可以避免训练出来的参数过大从而使模型过拟合。使用正则化缓解过拟合的手段广泛应用，不论是在线性回归还是在神经网络的梯度下降计算过程中，都应用到了正则化的方法。常用的正则化有l1正则和l2正则，具体使用哪个视具体情况而定，一般l2正则应用比较多

3. 减少特征数

•   欠拟合需要增加特征数，那么过拟合自然就要减少特征数。去除那些非共性特征，可以提高模型的泛化能力

4. 调整参数和超参数

•   不论什么情况，调参是必须的

5. 降低模型的复杂度

•   欠拟合要增加模型的复杂度，那么过拟合正好反过来

6. 使用Dropout

•   这一方法只适用于神经网络中，即按一定的比例去除隐藏层的神经单元，使神经网络的结构简单化

7. 提前结束训练

•   即early stopping，在模型迭代训练时候记录训练精度(或损失)和验证精度(或损失)，倘若模型训练的效果不再提高，比如训练误差一直在降低但是验证误差却不再降低甚至上升，这时候便可以结束模型训练了



二.没有使用任何正则化方法
正则化是现在训练神经网络一个非常重要的方法，通常是以 dropout、噪音或者其他某种随机过程的形式来加入到网络中。即便数据维度比参数更多，或者是在某种情况下不需要在意过拟合或者不可能出现过拟合，加入 dropout 或者某些形式的噪音仍然是很有帮助的。

正则化方法不仅仅是用于控制过拟合，通过在训练过程中引入一些随机过程，在某种程度上是"平滑"了成本格局。这可以加快训练收敛的速度，帮助处理数据中的噪声或异常值，并防止网络的极端权值配置。

最常用的正则化方法就是在卷积层或者全连接层之前采用 dropout 。一般会采用一个较高的概率，比如 0.75 或者 0.9，然后基于网络可能过拟合的概率来调整这个概率值，比如觉得不太可能出现过拟合，那么就把保留神经元的概率设置得非常高，比如 0.99。

数据增强或其他类型的噪音也可以像dropout一样实现正则化，有时候使用了足够的数据增强就可以不用 dropout。通常 dropout 被认为是将许多随机子网络的预测相结合的技术，但也可以将它视为一种数据增强的形式，在训练期间产生许多相似的输入数据变化。正如我们所知，避免过度拟合的最好方法是拥有足够多的数据，使得神经网络永远不会看到同样的数据两次！

最后，像训练神经网络其他方面一样，你需要小心你使用的正则化。请记住，在预测期间将其关闭，并注意，一旦它被关闭，您通常会得到略有不同的结果。在你需要极其精确的数字预测的情况下，某些形式的正则化有时会使这一切变得困难。



![image-20250210214507978](https://radiantheart.oss-cn-guangzhou.aliyuncs.com/myimage/20250210214508124.png)

那么，有优点就肯定有缺点：

1. 主体是表达式，那么函数功能就不会非常复杂，只能够实现简单的功能；
2. 在lambda中，是不支持异常处理的，那么当处理异常时，程序就会崩溃：







决策边界在图像上的呈现取决与样本数据在图像上的呈现，如果特征输入Z=W1+W2+W3+B,在这里因为只是一阶，所以只会是线性的，W1,W2,W3可以灵活去凑多项式，将决策边界实现到最为贴近图像的状态。
